<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Pose Tracking</title>

		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css"  id='theme'>

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/monokai.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section data-menu-title="">
					<h1>Pose Tracking</h1>
					<p>Djalma Lúcio Soares da Silva</p>
					<p><small>dsoares@inf.puc-rio.br</small></p>
					<p><small>INF2064 (2019.2)</small></p>
				</section>
				
				
				<section data-menu-title="O projeto">
					<h2>Projeto</h2>
					<section data-menu-title="Projeto">
						<h4>Pose Tracking</h4>
						<p style="text-align: justify;">
							Projeto final do curso INF2064:
							Topics in Computer Vision, Deep Neural Networks, Geometric Modeling and Rendering
						</p>
						<p style="font-size: 0.9em; margin-top: 10%;"> 
							Professor Marcelo Gattass
						</p>
					</section>

					<section data-menu-title="Introdução">
						<h5 style="text-align: left;">Introdução</h5>
						<p style="text-align: justify; font-size: 0.75em;">
							Estimação de pose (Pose Estimation) é um dos problemas tratados em Visão Computacional, 
							onde são detectadas a posição e orientação de um objeto. 
							Isso geralmente significa detectar os locais dos keypoints que descrevem o objeto.
						</p>
						<p style="text-align: justify; font-size: 0.75em;">
							Inferir a pose de diversas pessoas em uma imagem possui um conjunto de desafios a serem superados. 
							Primeiramente, cada imagem pode conter um número desconhecido de pessoas que podem estar em qualquer posição e escala. 
							Além disso, as interações entre as pessoas normalmente gera interferência, devido ao contato e oclusão, 
							dificultando a realização da associação das partes.
								
						</p>
					</section>

					<section data-menu-title="Objetivo">
						<h5 style="text-align: left;">Objetivo</h5>
						<p style="text-align: justify;">
							O objetivo do projeto é realizar o rastreamento e a 
							identificação da pose de diversas pessoas em tempo real no navegador. 
						</p>
					</section>

					<section data-menu-title="Motivação">
						<h5 style="text-align: left;">Motivação</h5>
						<p style="text-align: justify; font-size: 0.9em;">
							Atualmente há diversas aplicações que realizam a captura de movimento, 
							contudo é necessário o uso de câmeras especiais e vestimenta apropriada para realizar esta tarefa.
						</p>
						<p style="text-align: justify; font-size: 0.9em;">
							Este projeto tem como motivação a aplicação dos conhecimentos adquiridos durante sua implementação, 
							no contexto de animação de personagens humanos e humanóides, 
							através da captura de movimentos tanto no ambiente 2D como no 3D.
						</p>
					</section>

				</section>
				
				<section data-menu-title="Estimação de pose">
					<h3>Estimação de pose</h3>

					<section data-menu-title="Estimação de pose">
						<p style="text-align: justify; font-size: 0.9em;">
							Para o caso específico da estimação de pose de seres humanos,
							a estimação de pose refere-se às técnicas de visão computacional que detectam figuras humanas em imagens e vídeos, 
							para que se possa determinar, por exemplo, onde o cotovelo de alguém aparece em uma imagem.
						</p>
						<p style="text-align: justify; font-size: 0.9em;">
							Essa tecnologia não reconhece quem está em uma imagem, isto é, 
							não há informações pessoais identificáveis associadas à detecção de pose. 
							O algoritmo está simplesmente estimando onde estão as principais articulações do corpo.
						</p>
					</section>

					<section data-menu-title="Abordagens">
						<h5 style="text-align: left;">Abordagens</h5>
						<p style="text-align: justify;">
							A tarefa de detectar e localizar os keypoints de diversas pessoas em uma imagem
							é realizada através das seguintes abordagens:
							<ul>
								<li>Top-Down</li>
								<li>Bottom-Up</li>
							</ul>
						</p>
					</section>

					<section data-menu-title="Top-Down">
						<h5 style="text-align: left;">Top-Down</h5>
						<div style="display: flex;">
							<div style="width: 60%; margin-right: 5%;">
								<p style="text-align: justify; font-size: 0.60em;">
									A abordagem comum para estimar a pose é empregar um detector de pessoas, e então, 
									para cada pessoa detectada, realizar a estimativa da pose. 
									Esta abordagem é conhecida como abordagem top-down. 
									Um dos principais problemas com esta abordagem ocorre quando o detector de pessoas falha, 
									sendo assim não há como recuperar as informações necessárias (os keypoints) para a estimativa da pose.
								</p>
							</div>

							<div style="flex-grow: 1;">
								<img src="images/Top-Down.png" alt="" srcset="" style="margin-bottom: 0;">
								<p style="font-size: 0.3em; margin-top: 0; text-align: justify;">
									Todos as pessoas são detectas, para depois o keypoints serem detectados e feitas as ligações entre eles
								</p>
							</div>
						</div>
					</section>

					<section data-menu-title="Bottom-Up">
						<h5 style="text-align: left;">Bottom-Up</h5>
						<div style="display: flex;">
							<div style="width: 60%; margin-right: 5%;">
								<p style="text-align: justify; font-size: 0.70em;">
									Na abordagem bottom-up as partes são detectadas para todas os pessoas de uma única vez,
									sendo assim, não sofre o problema ocasionado pelo detector de pessoas. 
								</p>
							</div>

							<div style="flex-grow: 1;">
								<img src="images/Bottom-Up.png" alt="" srcset="" style="margin-bottom: 0;">
								<p style="font-size: 0.3em; margin-top: 0;">
									Todos os keypoints são detectados e em seguida são feitas as ligações entre eles
								</p>
							</div>
						</div>
					</section>					

					<section data-menu-title="Aplicações">
						<h5 style="text-align: left;">Aplicações</h5>
						<p style="text-align: justify; font-size: 0.80em;">
							A estimação de pose tem muitos usos, 
							desde instalações interativas que reagem ao corpo até realidade aumentada, 
							animação, usos de condicionamento físico e muito mais; tais como:
						</p>
						<p>
							<a href="https://experiments.withgoogle.com/experiments?tag=PoseNet" target="_blank" rel="noopener noreferrer">
								Experiments with Google
							</a>
						</p>
						<p>
							<a href="https://mayaontheinter.net/posenetsketchbook/" target="_blank" rel="noopener noreferrer">
								PoseNet Sketchbook
							</a>
						</p>
						<p>
							<a href="https://dev.to/devdevcharlie/playing-beat-saber-in-the-browser-with-body-movements-using-posenet-tensorflow-js-36km" target="_blank" rel="noopener noreferrer">
								Playing Beat Saber in the browser with body movements using PoseNet & Tensorflow.js
							</a>
						</p>
					</section>
				</section>


				<section data-menu-title="PoseNet - visão geral">
					<h3>PoseNet <sub><small>visão geral</small></sub></h3>

					<section data-menu-title="PoseNet - visão geral">
						<p style="text-align: justify; font-size: 1.0em;">
							PoseNet é um modelo de machine learning usado para fazer a estimação de pose de pessoas 
							em tempo real no navegador usando o TensorFlow.js.							
						</p> 
						<p style="text-align: justify; font-size: 1.0em;">
							Esta seção apresenta uma visão geral do modelo.
							Mais informações podem ser encontradas no repositório do
							<a href="" target="_blank" rel="noopener noreferrer">projeto no github</a>
							e no post 
							<a href="https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5" target="_blank" rel="noopener noreferrer">
								Real-time Human Pose Estimation in the Browser with TensorFlow.js
							</a>
						</p>
					</section>
					<section data-menu-title="Modos de execução">
						<p style="text-align: justify; font-size: 1.0em;">
							O modelo PoseNet possui os modos de execução single-pose e multi-pose. <br>
							Isto significa que ele pode ser usado para estimar a pose de somente uma única pessoa ou a pose de múltiplas pessoas
							em uma imagem ou vídeo. 
						</p>
						
					</section>

					<section data-menu-title="Single-pose">
						<h5 style="text-align: left; margin-top: 5%;">Single-pose</h5>
						<div style="display: flex;">
							<div style="width: 60%; margin-right: 5%;">
								<p style="text-align: justify; font-size: 0.6em;">
									A detecção de um única pessoa é mais rápida e mais simples.
									Esse modo é ideal para o caso onde somente há uma pessoa no centro da imagem ou vídeo.
								</p>
								<p style="text-align: justify; font-size: 0.6em;">
									A desvantagem deste modo é que se houver múltiplas pessoas na imagem, 
									os keypoints de todas as pessoas serão estimadas como se fossem da mesma pessoa. 
									Por exemplo, o braço esquerdo da pessoa nº 1 e o joelho direito da pessoa nº 2 
									podem ser confundidos pelo algoritmo como pertencendo à mesma pose.
								</p>
							</div>

							<div  style="flex-grow: 1;">
								<img src="images/posenet_single_person.gif" alt="" srcset="" style="margin-bottom: 0;">
								<p style="font-size: 0.3em; margin-top: 0;">
									Algoritmo executado no modo single-pose
								</p>
							</div>

						</div>
					</section>

					<section data-menu-title="Multi-pose">
						<h5 style="text-align: left; margin-top: 5%;">Multi-pose</h5>
						<div style="display: flex;">
							<div style="width: 60%; margin-right: 5%;">
								<p style="text-align: justify; font-size: 0.6em;">
									O algoritmo de estimativa de pose para várias pessoas
									é mais complexo e um pouco mais lento que o algoritmo de pose única.
								</p>
								<p style="text-align: justify; font-size: 0.6em;">
									Contudo, tem a vantagem de que, se várias pessoas aparecerem em uma imagem, 
									é menos provável que os keypoints detectados estejam associados à pose errada.
								</p>
								<p style="text-align: justify; font-size: 0.6em;">
									Outra vantagem é que o desempenho não é afetado pelo número de pessoas na imagem de entrada. 
									Isto é, se houver 15 pessoas para detectar ou 5, o tempo de computação será o mesmo.
								</p>
							</div>

							<div  style="flex-grow: 1;">
								<img src="images/posenet_multiple_person.gif" alt="" srcset="" style="margin-bottom: 0;">
								<p style="font-size: 0.3em; margin-top: 0;">
									Algoritmo executado no modo multi-pose
								</p>
							</div>

						</div>
					</section>

					<section data-menu-title="Com funciona">
						<h5 style="text-align: left; margin-top: 5%;">Como funciona</h5>
						<p style="text-align: justify; font-size: 0.9em;">
							A estimação de pose ocorre em duas fases:
							<ol style="text-align: justify; font-size: 0.9em;">
								<li>
									Uma imagem RGB é utilizada como entrada para a CNN do modelo
								</li>
								<li>
									Tanto no modo single-pose quanto no multi-pose, a saída do modelo é decodificada, obtendo-se:
									<ul>
										<li>as poses detectadas</li>
										<li>o score de confiança das poses detectadas</li>
										<li>a posição dos keypoints</li>
										<li>o score de confiança dos keypoints</li>
									</ul>
								</li>
							</ol>
						</p>
					</section>

					<section data-menu-title="Pose">
						<!-- <h5 style="text-align: left; margin-top: 5%;">Pose</h5> -->
						<div style="display: flex;">
							<div style="margin-right: 3%; flex: 0 0 50%">
								<p style="text-align: left; font-size: 0.90em;">
									<strong>Pose</strong>
								</p>
								<p style="text-align: justify; font-size: 0.60em;">
									O modelo PoseNet retorna um objeto de pose 
									que contém uma lista de keypoints e um score de confiança 
									para cada pessoa detectada.
								</p>

								<p style="text-align: left; font-size: 0.90em;">
									<strong>Score de confiança da pose</strong>
								</p>
								<p style="text-align: justify; font-size: 0.60em;">
									Determina a confiança de que a posição estimada do keypoint é precisa. 
									Seu valor varia entre 0.0 e 1.0.<br>
									Este valor pode ser usado para ocultar keypoints que não são considerados precisos o suficiente.
								</p>
							</div>

							<div  style="flex: 1 0  60%;">
								<img src="images/posenet_confidence_score_vs_keypoint_score.png" alt="" srcset="" style="margin-bottom: 0; margin-top: 15%;">
								<!-- <p style="font-size: 0.25em; margin-top: 0;">
									Fonte: <a href="https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5" target="_blank" rel="noopener noreferrer">
										<i>Real-time Human Pose Estimation in the Browser with TensorFlow.js</i>
									</a>
								</p> -->
							</div>
							
						</div>
					</section>

					<section data-menu-title="Keypoints">
						<h5 style="text-align: left; margin-top: 5%; font-size: 0.80em;">Keypoints</h5>
						<div style="display: flex;">
							<div style="margin-right: 5%; flex: 0 0 50%">
								<p style="text-align: justify; font-size: 0.65em;">
									Cada keypoint é uma parte da pose estimada de uma pessoa,
									como nariz, orelha direita, joelho esquerdo, pé direito, etc.<br><br>
									Nele está contida sua posição e o seu score de confiança.<br><br>
									Atualmente, o modelo PoseNet detecta 17 keypoints.
								</p>
							</div>

							<div  style="flex: 1 0 60%;">
								<img src="images/posenet_keypoints.png" alt="" srcset="" style="margin-bottom: 0;">
								<!-- <p style="font-size: 0.25em; margin-top: 0;">
									Fonte: <a href="https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5" target="_blank" rel="noopener noreferrer">
										<i>Real-time Human Pose Estimation in the Browser with TensorFlow.js</i>
									</a>
								</p> -->
							</div>
							
						</div>
					</section>
				</section>


				<section data-menu-title="PoseNet - visão técnica">
					<h3>PoseNet <sub><small>visão técnica</small></sub></h3>

					<section data-menu-title="PoseNet - visão técnica">
						<h5 style="text-align: left; margin-top: 5%; font-size: 0.80em;">Processo de estimação</h5>
						<div style="display: flex;">
							<div style="margin-right: 5%; flex: 0 0 50%">
								<p style="text-align: justify; font-size: 0.60em;">
									A imagem ao lado ilustra o processo de estimação de pose.

									Existem duas versões do modelo PoseNet, uma delas foi treinada 
									usando como backbone a MobileNet V1 e a outra ResNet 50.
								</p>
								<p style="text-align: justify; font-size: 0.60em;">

									A versão com a ResNet possui maior precisão, 
									contudo ela é bem maior e possui muitas camadas, 
									o que torna mais lento o carregamento da página. 
								</p>
								<p style="text-align: justify; font-size: 0.60em;">

									Sendo assim, a versão com MobileNet é a ideal para as aplicações de tempo real 
									que irão rodar em dispositivos móveis e computadores menos potentes.
										
								</p>
							</div>

							<div  style="flex: 1 0 60%;">
								<img src="images/posenet_single_person_pose_detector_pipeline.png" alt="" srcset="" style="margin-bottom: 0;">
								<!-- <p style="font-size: 0.25em; margin-top: 0;">
									Pipeline de estimação de pose<br>
									Fonte: <a href="https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5" target="_blank" rel="noopener noreferrer">
										<i>Real-time Human Pose Estimation in the Browser with TensorFlow.js</i>
									</a>
								</p> -->
							</div>
							
						</div>
					</section>

					<section data-menu-title="Output stride">
						<h5 style="text-align: left; margin-top: 5%; margin-bottom:0; font-size: 0.70em;">Output stride</h5>
						<div style="display: flex;">
							<div style="margin-right: 5%; flex: 0 0 50%">
								<p style="text-align: justify; font-size: 0.50em;">
									O modelo PoseNet é invariante com relação ao tamanho da imagem, 
									isto significa que ele pode estimar a pose independente da escala da imagem. 
									
								</p>
								<p style="text-align: justify; font-size: 0.50em;">
									Sendo assim o modelo pode  ter sua precisão aumentada apenas configurando 
									o parâmetro output stride em tempo de execução.  
									Lembrando que aumentar a precisão diminui o desempenho. 
									
								</p>
								<p style="text-align: justify; font-size: 0.50em;">

									O parâmetro output stride determina o quanto a saída será diminuída em relação ao tamanho da imagem de entrada.
									Este parâmetro influencia no tamanho dos layers da rede e saídas, e na precisão.
								</p>
								<p style="text-align: justify; font-size: 0.50em;">
									O output stride pode ser configurado com os valores 8, 16 ou 32. 
									Com o valor 32, o desempenho aumenta porém a precisão diminui. 
									Já com o valor 8 a precisão aumenta mas o desempenho diminui. 
										
								</p>
							</div>

							<div  style="flex: 1 0 60%;">
								<img src="images/posenet_output_stride.png" alt="" srcset="" style="margin-bottom: 0;">
								<!-- <p style="font-size: 0.25em; margin-top: 0;">
									Fonte: <a href="https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5" target="_blank" rel="noopener noreferrer">
										<i>Real-time Human Pose Estimation in the Browser with TensorFlow.js</i>
									</a>
								</p> -->
							</div>
							
						</div>
					</section>

					<section data-menu-title="Heatmaps & Offset Vectors (I)" style="margin-left: 0;">
						<h5 style="text-align: left; margin-top: 5%; margin-bottom:0; font-size: 0.62em;">heatmaps & offset vectors</h5>
						<div style="display: flex;">
							<div style="margin-left: -10%; flex: 0 0 60%;">
								<p style="text-align: justify; font-size: 0.55em;">
									As saídas do modelo PoseNet são um heatmap e um offset vector que devem ser 
									decodificados para que seja encontrada a área da imagem onde há a maior probabilidade de haver keypoints da pose estimada. 
									Isto significa que os keypoints da pose estimada estão associados a um tensor para heatmap e um tensor para o offset vector. 
									
								</p>
								<p style="text-align: justify; font-size: 0.55em;">
									Tanto o heatmap quanto o offset vector são tensores 3D com uma determinada altura e comprimento que são referenciadas como resolução. 
									A resolução é definida através do tamanho da imagem e do valor do parâmetro output stride.																		
									
								</p>								
							</div>

							<div  style="flex: 1; margin-left: -10%;">
								<pre style="width: 60%;">
									<code data-trim data-noescape  style="font-size: 0.55em;">
										Resolution = ((InputImageSize - 1) / OutputStride) + 1
										// Example: an input image with a width of 225 pixels and an output
										// stride of 16 results in an output resolution of 15
										// 15 = ((225 - 1) / 16) + 1
										
									</code>
								</pre>
							</div>
							
						</div>
						
					</section>
					<section>
						<!-- Slide com a figura do heatmap e offset vector -->
						<img src="images/posenet_heatmap_offset_vector.png" alt="" srcset="" style="margin-top: 5%;">
					</section>


					<section data-menu-title="Heatmaps & Offset Vectors (II)" style="margin-left: 0;">
						<div style="display: flex;">
							<div style="margin-left: -10%; flex: 0 0 60%;">
								<h5 style="text-align: left; margin-top: 5%; margin-bottom:0; font-size: 0.62em;">heatmaps</h5>
								<p style="text-align: justify; font-size: 0.55em;">
									Cada heatmap é um tensor 3D de tamanho resolução x resolução x 17, 
									onde 17 é o número de keypoints detectados pelo PoseNet.
								</p> 
								<p style="text-align: justify; font-size: 0.55em;">
									Por exemplo, uma imagem com tamanho de 225 e output stride de 16, resultará em um tensor de 15x15x17. 
									Cada fatia corresponde a um específico keypoint no heatmap. 
									Cada posição no headmap possui um score de confiança, 
									o qual é a probabilidade que o keypoint existe naquela posição. 
																			
								</p>
								<p style="text-align: justify; font-size: 0.55em;">
									Ela também pode ser vista como a imagem original sendo dividida em uma gride de 15x15, 
									onde os scores do heatmap fornecem uma classificação da probabilidade de cada keypoint existir em cada quadrado do gride.
								</p>								
							</div>

							<div  style="flex: 1 0 60%; margin-left: 5%;">
								<h5 style="text-align: left; margin-top: 5%; margin-bottom:0; font-size: 0.62em;">offset vectors</h5>

								<p style="text-align: justify; font-size: 0.55em;">
									Cada offset vector é um tensor 3D de tamanho resolução x resolução x 34, 
									onde 34 é o número de keypoints vezes 2. 									
								</p>
								<p style="text-align: justify; font-size: 0.55em;">
										Por exemplo, uma imagem com tamanho de 225 e output stride de 16, resultará em um tensor de 15x15x34. 
										Sabendo que os heatmaps são um aproximação de os keypoints estão, 
										os offset vectors correspondem em localização aos pontos do heatmap, 
										e são usados para fazer a predição da localização exata dos keypoints. 
								</p>
								<p style="text-align: justify; font-size: 0.55em;">
									Nas primeiras 17 fatias do offset vector estão os valores da coordenada x 
									e as últimas 17 fatias estão os valores da coordenada y.
								</p>
							</div>
							
						</div>
						
					</section>
					
					<section data-menu-title="Estimando a pose">
						<h5 style="text-align: left; margin-top: 5%; margin-left: -10%; font-size: 0.62em;">Estimando a pose</h5>
						<div style="display: flex;">
							<div style="margin-left: -10%; flex: 0 0 50%;">
								<p style="text-align: justify; font-size: 0.45em;">
									Depois que a imagem é processada pelo modelo, alguns cálculos são realizados para estimar a pose a partir das saídas. 
								</p> 
								<p style="text-align: justify; font-size: 0.45em;">
									Por exemplo, no modo single-pose é retornado o score da confiança da pose, 
									o qual contém um array de keypoints, indexados pelo ID da parte, 
									cada um contendo o score da confiança e as coordenadas x,y da posição.
								</p>
								<p style="text-align: justify; font-size: 0.45em;">
									Para obter os keypoints da pose:
								</p>
								<ol style="text-align: justify; font-size: 0.45em;">
									<li>
										Aplica-se uma ativação sigmoid no mapa de calor para se obter os scores
										<pre style="width: 42%;">
											<code data-trim data-noescape class="hljs" style="font-size: 1.3em;">
												scores = heatmap.sigmoid()
											</code>
										</pre>
									</li>
									<li>
										É aplicado o argmax2d no score de confiança do keypoint para obter os índices x e  y 
										no heatmap com o maior score de cada parte, ou seja é essencialmente onde a parte existe. 
										É retornado um tensor de tamanho 17x2, onde cada linha dele estão os índices y e x do heatmap. 
										<pre style="width: 62%;">
											<code data-trim data-noescape class="javascript" style="font-size: 1.3em;">
												heatmapPositions = scores.argmax(y, x)
											</code>
										</pre>
									</li>
								</ol>
							</div>

							<div  style="flex: 1; margin-left: 5%;">
								<p style="text-align: justify; font-size: 0.55em;">
									<ol start="3" style="text-align: justify; font-size: 0.45em;">
										<li>
											O offset vector de cada parte é recuperado ao usar  os índices x e y retornados 
											pelo objeto offset que usa os índices x e y correspondentes no heatmap para a parte.  
											Por exemplo, para a parte k, quando a posição no heatmap for x e y, o offset vector é:
											<pre style="width: 90%;">
												<code data-trim data-noescape class="javascript" style="font-size: 1.3em;">
													offsetVector = [offsets.get(y, x, k), offsets.get(y, x, 17 + k)]
												</code>
											</pre>
										</li>
										<li>
											Para obter o keypoint, os índices x e y do heatmap de cada parte são multiplicados pelo output stride 
											e depois adicionado ao seu offset vector correspondente, que está na mesma escala da imagem original.
											<pre style="width: 94%;">
												<code data-trim data-noescape class="javascript" style="font-size: 1.3em;">
													keypointPositions = heatmapPositions * outputStride + offsetVectors
												</code>
											</pre>
										</li>
										<li>
											O score da confiança de cada keypoint é o score da posição do seu heatmap. 
											E o score da confiança da pose é a média dos scores dos keypoints.
										</li>
									</ol>
								</p>
							</div>
							
						</div>

					</section>

				</section>


				<section data-menu-title="Arquitetura">
					<h3>Arquitetura</h3>
					<section data-menu-title="Arquitetura (TBD)">
						<div>
							<img src="images/arquitetura.png" alt="" srcset="">
						</div>
					</section>
					<section data-menu-title="Componentes">
						<ul>
							<li>PoseNet ...</li>
							<li>ml5.js ...</li>
							<li>OpenCV.js ...</li>
							<li>p5.js ...</li>
						</ul>
					</section>
					<!-- <section data-menu-title="PoseNet">PoseNet</section> -->
					<section data-menu-title="ml5.js">ml5.js</section>
				</section>

				<section>
					<h3>Experimentos</h3>
					<section data-menu-title="Experimentos">
						<p style="text-align: justify;">
							Para um melhor entendimento das bibliotecas utilizadas no projeto, 
							foram realizados os seguintes experimentos:
							<ul>
								<li><a href="../experiments/opencv/" target="_blank">OpenCV: Meanshift & Camshift</a></li>
								<li><a href="../experiments/lucas-kanade/" target="_blank">OpenCV: Lucas-Kanade Optical Flow</a></li>
								<li><a href="../experiments/posenet/" target="_blank">PoseNet</a></li>
								<li><a href="../experiments/poseshift/" target="_blank">PoseShift</a></li>
								<li><a href="../experiments/poseflow/" target="_blank">PoseFlow</a></li>
							</ul>
						</p>
					</section>
					<section>
						<h4><a href="../experiments/opencv/" target="_blank">OpenCV: Meanshift & Camshift</a></h4>
						<p style="text-align: justify;">
							O experimento <a href="../experiments/opencv/" target="_blank">OpenCV: Meanshift & Camshift</a> utiliza 
							os algoritmos de rastreamento 
							<a href="https://docs.opencv.org/trunk/df/def/tutorial_js_meanshift.html" target="_blank" rel="noopener noreferrer">
								Meanshift e Camshift 
							</a>
							fornecidos pela biblioteca <em>OpenCV.js</em>.<br><br>
							O código-fonte do experimento encontra-se disponível no
							<a href="https://github.com/dlucio/inf2064/blob/master/project/experiments/opencv/sketch.js" target="_blank" rel="noopener noreferrer">
								github do projeto
							</a>
						</p>
					</section>
					<section data-menu-title="OpenCV: Lucas-Kanade">
						<h4><a href="../experiments/lucas-kanade/" target="_blank">OpenCV: Lucas-Kanade Optical Flow</a></h4>
						<p style="text-align: justify;">
							O experimento <a href="../experiments/lucas-kanade/" target="_blank">OpenCV: Lucas-Kanade Optical Flow</a> 
							para realizar o rastreamento utiliza sparse optical flow através do método 
							<a href="https://docs.opencv.org/trunk/db/d7f/tutorial_js_lucas_kanade.html" target="_blank" rel="noopener noreferrer">
								Lucas-Kanade 
							</a>
							fornecido pela biblioteca <em>OpenCV.js</em>.<br><br>
							O código-fonte do experimento encontra-se disponível no
							<a href="https://github.com/dlucio/inf2064/blob/master/project/experiments/lucas-kanade/sketch.js" target="_blank" rel="noopener noreferrer">
								github do projeto
							</a>
						</p>
					</section>
					<section>
						<h4><a href="../experiments/posenet/" target="_blank">PoseNet</a></h4>
						<p style="text-align: justify;">
							O experimento <a href="../experiments/posenet/" target="_blank">PoseNet</a> permite 
							que sejam testados os diversos parâmentros utilizados pelo modelo PoseNet disponibilizado
							pela biblioteca <em>ml5.js</em>.<br><br>
							O código-fonte do experimento encontra-se disponível no
							<a href="https://github.com/dlucio/inf2064/blob/master/project/experiments/posenet/sketch.js" target="_blank" rel="noopener noreferrer">
								github do projeto
							</a>
						</p>
					</section>
					<section>
						<h4><a href="../experiments/poseshift/" target="_blank">PoseShift</a></h4>
						<p style="text-align: justify;">
							O experimento <a href="../experiments/poseshift/" target="_blank">PoseShift</a>
							é a integração entre os experimentos OpenCV: Meanshift & Camshift e PoseNet. <br><br>
							O código-fonte do experimento encontra-se disponível no
							<a href="https://github.com/dlucio/inf2064/blob/master/project/experiments/poseshift/sketch.js" target="_blank" rel="noopener noreferrer">
								github do projeto
							</a>
						</p>
					</section>
					<section>
						<h4><a href="../experiments/poseflow/" target="_blank">PoseFlow</a></h4>
						<p style="text-align: justify;">
							O experimento <a href="../experiments/poseflow/" target="_blank">PoseFlow</a>
							é a integração entre os experimentos OpenCV: Lucas-Kanade Optical Flow e PoseNet.<br><br>
							O código-fonte do experimento encontra-se disponível no
							<a href="https://github.com/dlucio/inf2064/blob/master/project/experiments/poseflow/sketch.js" target="_blank" rel="noopener noreferrer">
								github do projeto
							</a>
						</p>
					</section>					
				</section>
				
				<section data-menu-title="Pose Tracking">
					<h3>
						Pose Tracking
					</h3>
					<section data-menu-title="Pose Tracking">
						<p style="text-align: justify;">
							No projeto <a href="../posetracking/" target="_blank">Pose Tracking</a>  
							é utilizado o modelo 
							<a href="https://github.com/tensorflow/tfjs-models/tree/master/posenet" target="_blank">PoseNet</a> 
							para detectar a pose das diversas pessoas, 
							e para o rastreamento e identificação de cada pose detectada é utilizado o algoritmo 
							<a href="https://www.pyimagesearch.com/2018/07/23/simple-object-tracking-with-opencv/" target="_blank">Centroid Tracking</a>.
							<br><br>
							O código-fonte encontra-se disponível no
							<a href="https://github.com/dlucio/inf2064/tree/master/project/posetracking" target="_blank" rel="noopener noreferrer">
								github do projeto</a>.
						</p>
					</section>

					<section data-menu-title="Centroid Tracking">
						<h5 style="text-align: left; margin-top: 5%;">
							Centroid Tracking
						</h5>
						<p style="text-align: justify; font-size: 0.75em;">
							A implementação do algoritmo Centroid Tracking neste trabalho é uma conversão para Javascript
							da implementação em Python encontrada e explicada com detalhes no post 
							<a href="https://www.pyimagesearch.com/2018/07/23/simple-object-tracking-with-opencv/" target="_blank">
								Simple object tracking with OpenCV.
							</a>
							<br><br>
							Esse algoritmo é chamado de Centroid Tracking, pois se baseia na distância euclidiana entre 
							(1) centróide de objeto existente, ou seja, objetos que o algoritmo já havia visto antes e 
							(2) novos centróides de objeto entre frames subsequentes em um vídeo.
							<br><br>
							O algoritmo é processado em várias etapas conforme apresentado a seguir.
						</p>
					</section>

					<section data-menu-title="Centroid Tracking: Etapa 1">
						<h5 style="text-align: center; font-size: 0.80em;">
							Etapa 1: Obter as coordenadas do bounding box e calcular os centróides
						</h5>
						<div style="display: flex;">
							<div style="margin-left: -10%; flex: 0 0 60%;">
								<p style="text-align: justify; font-size: 0.60em;">
									A cada frame o algoritmo recebe as coordenadas da bounding box de cada objeto detectado no frame.
									<br><br>
									Estas bounding boxes são geradas a partir das coordenadas dos keypoints detectados pelo modelos PoseNet.
									<br><br>
									Estas coordenadas são utilizadas para calcular o centróide, ou seja, o centro da bounding box.
									<br><br>
									Como esse é o conjunto inicial de bounding boxes apresentado ao algoritmo, então, nesse momento, 
									são criados os identificadores únicos dos objetos.
								</p>
							</div>

							<div  style="flex: 1 0 45%; margin-left: 5%;">
								<img src="images/simple_object_tracking_step1.png" alt="" srcset="">
								<p style="font-size: 0.35em; margin-top: 0;">
									Bounding boxes usadas para calcular os centróides
								</p>
							</div>
						</div>
					</section>

					<section data-menu-title="Centroid Tracking: Etapa 2">
						<h5 style="text-align: center; font-size: 0.80em;">
							Etapa 2: Cálculo da distância Euclidiana entre as novas bounding boxes e os objetos existentes
						</h5>
						<div style="display: flex;">
							<div style="margin-left: -10%; flex: 0 0 60%;">
								<p style="text-align: justify;  font-size: 0.60em;">
									Em cada frame subsequente é aplicada a Etapa 1 para calcular os centróides dos objetos; contudo,
									ao invés de atribuir um identificador único para cada objeto detectado, 
									primeiro é verificado se é possivel associar o centróide de um novo objeto (em amarelo) 
									com o centróide de um objeto antigo (em rosa).
									<br><br>
									Para realizar este processo, é calculada a distância Euclidiana (setas verdes) 
									entre cada par dos centróides dos objetos existentes e os centróides dos objetos de entrada.
								</p>								
							</div>

							<div  style="flex: 1 0 45%; margin-left: 5%;">
								<img src="images/simple_object_tracking_step2.png" alt="" srcset="">
								<p style="font-size: 0.35em; margin-top: 0;">
									Distância Euclidiana calculada entre os centróides existentes (vermelho) e os novos (verde)
								</p>
							</div>
						</div>
					</section>

					<section data-menu-title="Centroid Tracking: Etapa 3">
						<h5 style="text-align: center; font-size: 0.80em;">
							Etapa 3: Atualizando as coordenadas do objetos existentes
						</h5>
						<div style="display: flex;">
							<div style="margin-left: -10%; flex: 0 0 60%;">
								<p style="text-align: justify;  font-size: 0.65em;">
									A principal suposição usada pelo algoritmo Centroid Tracking é de que, provavelmente, 
									um objeto irá se mover entre os frames subsequentes, mas a distância entre os centróides 
									nos frames F<sub>t</sub> e F<sub>t+1</sub>
									será menor do que todas as outras distâncias entre os objetos.
									<br><br>
									Portanto, se optarmos por associar os centróides com distâncias mínimas entre 
									os frames subseqüentes, teremos o tracking dos objetos.
																				
								</p>
							</div>

							<div  style="flex: 1 0 45%; margin-left: 5%;">
								<img src="images/simple_object_tracking_step3.png" alt="" srcset="">
								<p style="font-size: 0.35em; margin-top: 0;">
									O algoritmo associando centróides que minimizam suas respectivas distâncias euclidianas.
								</p>
							</div>
						</div>
					</section>

					<section data-menu-title="Centroid Tracking: Passo 4">
						<h5 style="text-align: center; font-size: 0.80em; margin-top: 3%;">
							Etapa 4: Registrando novos objetos
						</h5>
						<div style="display: flex;">
							<div style="margin-left: -10%; flex: 0 0 60%;">
								<p style="text-align: justify;  font-size: 0.65em;">
									No caso em que houver mais detecções de entrada do que 
									objetos existentes sendo rastreados, é necessário registrar o novo objeto. <br><br>
									"Registrar" significa simplesmente que o novo objeto será adicionado à lista de objetos rastreados,
									realizando as seguintes ações:
									<ol style="font-size: 0.60em; margin-left: 1.5em;">
										<li>
											Atribuindo um novo ID de objeto
										</li>
										<li>
											Armazenando o centróide das coordenadas da bounding box para esse objeto
										</li>
									</ol>

								</p>
								<p style="text-align: justify;  font-size: 0.65em;">
									Após feito o registro, a Etapa 2 é executada e o pipeline é repetido para cada frame no stream de vídeo.
								</p>
							</div>

							<div  style="flex: 1 0 45%; margin-left: 5%;">
								<img src="images/simple_object_tracking_step4.png" alt="" srcset="">
								<p style="font-size: 0.35em; margin-top: 0;">
									A figura demonstra o processo de usar as distâncias euclidianas mínimas 
									para associar IDs de objetos existentes e, em seguida, registrar um novo objeto.
								</p>
							</div>
						</div>
					</section>

					<section data-menu-title="Centroid Tracking: Passo 5">
						<h5 style="text-align: center; font-size: 0.80em;">
							Passo 5: Cancelar o registro de objetos antigos
						</h5>
						<p style="text-align: justify;  font-size: 0.75em;">
								Qualquer algoritmo de rastreamento de objeto razoável precisa ser capaz de lidar com a perda, 
								o desaparecimento ou a saída de um objeto do campo de visão.
								<br><br>
								A maneira exata de lidar com essas situações depende realmente de como o tracker de objeto é implementado.
								<br><br>
								Nesta implementação, o registro de objetos antigos é cancelado quando eles não puderem corresponder 
								a nenhum objeto existente para um total de N frames subsequentes.
						</p>
					</section>

				</section>

				<!-- <section>
					<h4>Contribuições</h4>
					<ul style="font-size: 0.8em;">
						<li>Removido memory leak do ml5.PoseNet</li>
						<li>Adicionada bounding box ao ml5.PoseNet</li>
						<li>Primeiro exemplo utilizando ml5.PoseNet, OpenCV.js e p5js</li>
						<li>Corrigido os valores dos parâmetros de incialização do ml5.PoseNet.
							<ul>
								<li><em>Na documentação informava alguns valores mas no código os valores dos parâmentros eram outros</em></li> 							
							</ul>
						</li>
						<li>Adicionada a opção de habilitar e desabilitar a inferência no ml5.PoseNet.
							<ul>
								<li><em>Isso é útil quando é usada a interface de eventos para receber as poses detectadas.</em></li>
							</ul>
						</li>
					</ul>
				</section> -->

				<section>
					<h4>Trabalhos futuros</h4>
					<ul>
						<li>Utilizar o algoritmo Lucas-Kanade para fazer o tracking dos keypoints de diversas pessoas</li>
						<li>Utilizar o filtro de Kalman para dar mais estabilidade ao rastreamento</li>
					</ul>
				</section>
				
				<section>
					<h4>Conclusão</h4>
				</section>
			</div>
		</div>

		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				menu: {
					numbers: false,
					openSlideNumber: true,
					themes: true,
					themesPath: 'css/theme/',
					transitions: true,
					hideMissingTitles: true,
				},
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true },
					{ src: 'plugin/reveal.js-menu/menu.js', async: true },
					// Zoom in and out with Alt+click
					{ src: 'plugin/zoom-js/zoom.js', async: true },
				]
			});
		</script>
	</body>
</html>
