<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Pose Tracking</title>

		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css"  id='theme'>

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/monokai.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section data-menu-title="">
					<h1>Pose Tracking</h1>
					<p>Djalma Lúcio Soares da Silva</p>
					<p><small>dsoares@inf.puc-rio.br</small></p>
					<p><small>INF2064 (2019.2)</small></p>
				</section>
				
				
				<section data-menu-title="O projeto">
					<h2>Projeto</h2>
					<section data-menu-title="Projeto">
						<h4>Pose Tracking</h4>
						<p style="text-align: justify;">
							Projeto final do curso INF2064:
							Topics in Computer Vision, Deep Neural Networks, Geometric Modeling and Rendering
						</p>
						<p style="font-size: 0.9em; margin-top: 10%;"> 
							Professor Marcelo Gattass
						</p>
					</section>

					<section data-menu-title="Introdução">
						<h5 style="text-align: left;">Introdução</h5>
						<p style="text-align: justify; font-size: 0.75em;">
							Estimação de pose (Pose Estimation) é um dos problemas tratados em Visão Computacional, 
							onde são detectadas a posição e orientação de um objeto. 
							Isso geralmente significa detectar os locais dos keypoints que descrevem o objeto.
						</p>
						<p style="text-align: justify; font-size: 0.75em;">
							Inferir a pose de diversas pessoas em uma imagem possui um conjunto de desafios a serem superados. 
							Primeiramente, cada imagem pode conter um número desconhecido de pessoas que podem estar em qualquer posição e escala. 
							Além disso, as interações entre as pessoas normalmente gera interferência, devido ao contato e oclusão, 
							dificultando a realização da associação das partes.
								
						</p>
					</section>

					<section data-menu-title="Objetivo">
						<h5 style="text-align: left;">Objetivo</h5>
						<p style="text-align: justify;">
							O objetivo do projeto é realizar o rastreamento e a 
							identificação da pose de diversas pessoas em tempo real no navegador. 
						</p>
					</section>

					<section data-menu-title="Motivação">
						<h5 style="text-align: left;">Motivação</h5>
						<p style="text-align: justify; font-size: 0.9em;">
							Atualmente há diversas aplicações que realizam a captura de movimento, 
							contudo é necessário o uso de câmeras especiais e vestimenta apropriada para realizar esta tarefa.
						</p>
						<p style="text-align: justify; font-size: 0.9em;">
							Este projeto tem como motivação a aplicação dos conhecimentos adquiridos durante sua implementação, 
							no contexto de animação de personagens humanos e humanóides, 
							através da captura de movimentos tanto no ambiente 2D como no 3D.
						</p>
					</section>

				</section>
				
				<section data-menu-title="Estimação de pose">
					<h3>Estimação de pose</h3>

					<section data-menu-title="Estimação de pose">
						<p style="text-align: justify; font-size: 0.9em;">
							Para o caso específico da estimação de pose de seres humanos,
							a estimação de pose refere-se às técnicas de visão computacional que detectam figuras humanas em imagens e vídeos, 
							para que se possa determinar, por exemplo, onde o cotovelo de alguém aparece em uma imagem.
						</p>
						<p style="text-align: justify; font-size: 0.9em;">
							Essa tecnologia não reconhece quem está em uma imagem, isto é, 
							não há informações pessoais identificáveis associadas à detecção de pose. 
							O algoritmo está simplesmente estimando onde estão as principais articulações do corpo.
						</p>
					</section>

					<section data-menu-title="Abordagens">
						<h5 style="text-align: left;">Abordagens</h5>
						<p style="text-align: justify;">
							A tarefa de detectar e localizar os keypoints de diversas pessoas em uma imagem
							é realizada através das seguintes abordagens:
							<ul>
								<li>Top-Down</li>
								<li>Bottom-Up</li>
							</ul>
						</p>
					</section>

					<section data-menu-title="Top-Down">
						<h5 style="text-align: left;">Top-Down</h5>
						<div style="display: flex;">
							<div style="width: 50%; margin-right: 5%;">
								<p style="text-align: justify; font-size: 0.60em;">
									A abordagem comum para estimar a pose é empregar um detector de pessoas, e então, 
									para cada pessoa detectada, realizar a estimativa da pose. 
									Esta abordagem é conhecida como abordagem top-down. 
									Um dos principais problemas com esta abordagem ocorre quando o detector de pessoas falha, 
									sendo assim não há como recuperar as informações necessárias (os keypoints) para a estimativa da pose.
								</p>
							</div>

							<div style="flex-grow: 1;">
								<img src="images/Top-Down.png" alt="" srcset="" style="margin-bottom: 0;">
								<p style="font-size: 0.3em; margin-top: 0; text-align: justify;">
									Todos as pessoas são detectas, para depois o keypoints serem detectados e feitas as ligações entre eles
								</p>
							</div>
						</div>
					</section>

					<section data-menu-title="Bottom-Up">
						<h5 style="text-align: left;">Bottom-Up</h5>
						<div style="display: flex;">
							<div style="width: 50%; margin-right: 5%;">
								<p style="text-align: justify; font-size: 0.70em;">
									Na abordagem bottom-up as partes são detectadas para todas os pessoas de uma única vez,
									sendo assim, não sofre o problema ocasionado pelo detector de pessoas. 
								</p>
							</div>

							<div style="flex-grow: 1;">
								<img src="images/Bottom-Up.png" alt="" srcset="" style="margin-bottom: 0;">
								<p style="font-size: 0.3em; margin-top: 0;">
									Todos os keypoints são detectados e em seguida são feitas as ligações entre eles
								</p>
							</div>
						</div>
					</section>					

					<section data-menu-title="Aplicações">
						<h5 style="text-align: left;">Aplicações</h5>
						<p style="text-align: justify; font-size: 0.80em;">
							A estimação de pose tem muitos usos, 
							desde instalações interativas que reagem ao corpo até realidade aumentada, 
							animação, usos de condicionamento físico e muito mais; tais como:
						</p>
						<p>
							<a href="https://experiments.withgoogle.com/experiments?tag=PoseNet" target="_blank" rel="noopener noreferrer">
								Experiments with Google
							</a>
						</p>
						<p>
							<a href="https://mayaontheinter.net/posenetsketchbook/" target="_blank" rel="noopener noreferrer">
								PoseNet Sketchbook
							</a>
						</p>
						<p>
							<a href="https://dev.to/devdevcharlie/playing-beat-saber-in-the-browser-with-body-movements-using-posenet-tensorflow-js-36km" target="_blank" rel="noopener noreferrer">
								Playing Beat Saber in the browser with body movements using PoseNet & Tensorflow.js
							</a>
						</p>
					</section>
				</section>


				<section data-menu-title="PoseNet">
					<h3>PoseNet</h3>

					<section data-menu-title="PoseNet">
						<p style="text-align: justify; font-size: 1.0em;">
							PoseNet é um modelo de machine learning usado para fazer a estimação de pose de pessoas 
							em tempo real no navegador usando o TensorFlow.js.							
						</p> 
						<p style="text-align: justify; font-size: 1.0em;">
							Esta seção apresenta uma visão geral do modelo.
							Mais informações podem ser encontradas no repositório do
							<a href="" target="_blank" rel="noopener noreferrer">projeto no github</a>
							e no post 
							<a href="https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5" target="_blank" rel="noopener noreferrer">
								Real-time Human Pose Estimation in the Browser with TensorFlow.js
							</a>
						</p>
					</section>
					<section data-menu-title="Modos de execução">
						<p style="text-align: justify; font-size: 1.0em;">
							O modelo PoseNet possui os modos de execução single-pose e multi-pose. <br>
							Isto significa que ele pode ser usado para estimar a pose de somente uma única pessoa ou a pose de múltiplas pessoas
							em uma imagem ou vídeo. 
						</p>
						
					</section>

					<section data-menu-title="Single-pose">
						<h5 style="text-align: left; margin-top: 5%;">Single-pose</h5>
						<div style="display: flex;">
							<div style="width: 50%; margin-right: 5%;">
								<p style="text-align: justify; font-size: 0.6em;">
									A detecção de um única pessoa é mais rápida e mais simples.
									Esse modo é ideal para o caso onde somente há uma pessoa no centro da imagem ou vídeo.
								</p>
								<p style="text-align: justify; font-size: 0.6em;">
									A desvantagem deste modo é que se houver múltiplas pessoas na imagem, 
									os keypoints de todas as pessoas serão estimadas como se fossem da mesma pessoa. 
									Por exemplo, o braço esquerdo da pessoa nº 1 e o joelho direito da pessoa nº 2 
									podem ser confundidos pelo algoritmo como pertencendo à mesma pose.
								</p>
							</div>

							<div  style="flex-grow: 1;">
								<img src="images/posenet_single_person.gif" alt="" srcset="" style="margin-bottom: 0;">
								<p style="font-size: 0.3em; margin-top: 0;">
									Algoritmo executado no modo single-pose
								</p>
							</div>

						</div>
					</section>

					<section data-menu-title="Multi-pose">
						<h5 style="text-align: left; margin-top: 5%;">Multi-pose</h5>
						<div style="display: flex;">
							<div style="width: 50%; margin-right: 5%;">
								<p style="text-align: justify; font-size: 0.6em;">
									O algoritmo de estimativa de pose para várias pessoas
									é mais complexo e um pouco mais lento que o algoritmo de pose única.
								</p>
								<p style="text-align: justify; font-size: 0.6em;">
									Contudo, tem a vantagem de que, se várias pessoas aparecerem em uma imagem, 
									é menos provável que os keypoints detectados estejam associados à pose errada.
								</p>
								<p style="text-align: justify; font-size: 0.6em;">
									Outra vantagem é que o desempenho não é afetado pelo número de pessoas na imagem de entrada. 
									Isto é, se houver 15 pessoas para detectar ou 5, o tempo de computação será o mesmo.
								</p>
							</div>

							<div  style="flex-grow: 1;">
								<img src="images/posenet_multiple_person.gif" alt="" srcset="" style="margin-bottom: 0;">
								<p style="font-size: 0.3em; margin-top: 0;">
									Algoritmo executado no modo multi-pose
								</p>
							</div>

						</div>
					</section>

					<section data-menu-title="Com funciona">
						<h5 style="text-align: left; margin-top: 5%;">Como funciona</h5>
						<p style="text-align: justify; font-size: 0.9em;">
							A estimação de pose ocorre em duas fases:
							<ol style="text-align: justify; font-size: 0.9em;">
								<li>
									Uma imagem RGB é utilizada como entrada para a CNN do modelo
								</li>
								<li>
									Tanto no modo single-pose quanto no multi-pose, a saída do modelo é decodificada, obtendo-se:
									<ul>
										<li>as poses detectadas</li>
										<li>o score de confiança das poses detectadas</li>
										<li>a posição dos keypoints</li>
										<li>o score de confiança dos keypoints</li>
									</ul>
								</li>
							</ol>
						</p>
					</section>

					<section data-menu-title="Pose">
						<!-- <h5 style="text-align: left; margin-top: 5%;">Pose</h5> -->
						<div style="display: flex;">
							<div style="width: 100%; margin-right: 3%;">
								<p style="text-align: left; font-size: 0.73em;">
									<strong>Pose</strong>
								</p>
								<p style="text-align: justify; font-size: 0.60em;">
									O modelo PoseNet retorna um objeto de pose 
									que contém uma lista de keypoints e um score de confiança 
									para cada pessoa detectada.
								</p>

								<p style="text-align: left; font-size: 0.72em;">
									<strong>Score de confiança da pose</strong>
								</p>
								<p style="text-align: justify; font-size: 0.60em;">
									Determina a confiança de que a posição estimada do keypoint é precisa. 
									Seu valor varia entre 0.0 e 1.0.<br>
									Este valor pode ser usado para ocultar keypoints que não são considerados precisos o suficiente.
								</p>
							</div>

							<div  style="flex-grow: 1;">
								<img src="images/posenet_confidence_score_vs_keypoint_score.png" alt="" srcset="" style="margin-bottom: 0; margin-top: 25%;">
								<p style="font-size: 0.25em; margin-top: 0;">
									Fonte: <a href="https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5" target="_blank" rel="noopener noreferrer">
										<i>Real-time Human Pose Estimation in the Browser with TensorFlow.js</i>
									</a>
								</p>
							</div>
							
						</div>
					</section>

					<section data-menu-title="Keypoints">
						<h5 style="text-align: left; margin-top: 5%;">Keypoints</h5>
						<div style="display: flex;">
							<div style="width: 100%; margin-right: 5%;">
								<p style="text-align: justify; font-size: 0.62em;">
									Cada keypoint é uma parte da pose estimada de uma pessoa,
									como nariz, orelha direita, joelho esquerdo, pé direito, etc.<br><br>
									Nele está contida sua posição e o seu score de confiança.<br><br>
									Atualmente, o modelo PoseNet detecta 17 keypoints.
								</p>
							</div>

							<div  style="flex-grow: 1;">
								<img src="images/posenet_keypoints.png" alt="" srcset="" style="margin-bottom: 0;">
								<p style="font-size: 0.25em; margin-top: 0;">
									Fonte: <a href="https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5" target="_blank" rel="noopener noreferrer">
										<i>Real-time Human Pose Estimation in the Browser with TensorFlow.js</i>
									</a>
								</p>
							</div>
							
						</div>
					</section>
				</section>

				<section data-menu-title="Arquitetura">
					<h3>Arquitetura</h3>
					<section data-menu-title="Arquitetura (TBD)">
						<div>
							<img src="images/arquitetura.png" alt="" srcset="">
						</div>
					</section>
					<section data-menu-title="Componentes">
						<ul>
							<li>PoseNet ...</li>
							<li>ml5.js ...</li>
							<li>OpenCV.js ...</li>
							<li>p5.js ...</li>
						</ul>
					</section>
					<!-- <section data-menu-title="PoseNet">PoseNet</section> -->
					<section data-menu-title="ml5.js">ml5.js</section>
				</section>

				<section>
					<h3>Experimentos</h3>
					<section data-menu-title="Experimentos">
						<p style="text-align: justify;">
							Para um melhor entendimento das bibliotecas utilizadas no projeto, 
							foram realizados os seguintes experimentos:
							<ul>
								<li><a href="../experiments/opencv/" target="_blank">OpenCV: Meanshift & Camshift</a></li>
								<li><a href="../experiments/lucas-kanade/" target="_blank">OpenCV: Lucas-Kanade Optical Flow</a></li>
								<li><a href="../experiments/posenet/" target="_blank">PoseNet</a></li>
								<li><a href="../experiments/poseshift/" target="_blank">PoseShift</a></li>
								<li><a href="../experiments/poseflow/" target="_blank">PoseFlow</a></li>
							</ul>
						</p>
					</section>
					<section>
						<h4><a href="../experiments/opencv/" target="_blank">OpenCV: Meanshift & Camshift</a></h4>
						<p style="text-align: justify;">
							O experimento <a href="../experiments/opencv/" target="_blank">OpenCV: Meanshift & Camshift</a> utiliza 
							os algoritmos de rastreamento 
							<a href="https://docs.opencv.org/trunk/df/def/tutorial_js_meanshift.html" target="_blank" rel="noopener noreferrer">
								Meanshift e Camshift 
							</a>
							fornecidos pela biblioteca <em>OpenCV.js</em>.<br>
							O código-fonte do experimento encontra-se disponível no
							<a href="https://github.com/dlucio/inf2064/blob/master/project/experiments/opencv/sketch.js" target="_blank" rel="noopener noreferrer">
								github do projeto
							</a>
						</p>
					</section>
					<section>
						<h4><a href="../experiments/lucas-kanade/" target="_blank">OpenCV: Lucas-Kanade Optical Flow</a></h4>
						<p style="text-align: justify;">
							O experimento <a href="../experiments/lucas-kanade/" target="_blank">OpenCV: Lucas-Kanade Optical Flow</a> 
							para realizar o rastreamento utiliza sparse optical flow através do método 
							<a href="https://docs.opencv.org/trunk/db/d7f/tutorial_js_lucas_kanade.html" target="_blank" rel="noopener noreferrer">
								Lucas-Kanade 
							</a>
							fornecido pela biblioteca <em>OpenCV.js</em>.<br>
							O código-fonte do experimento encontra-se disponível no
							<a href="https://github.com/dlucio/inf2064/blob/master/project/experiments/lucas-kanade/sketch.js" target="_blank" rel="noopener noreferrer">
								github do projeto
							</a>
						</p>
					</section>
					<section>
						<h4><a href="../experiments/posenet/" target="_blank">PoseNet</a></h4>
						<p style="text-align: justify;">
							O experimento <a href="../experiments/posenet/" target="_blank">PoseNet</a> permite 
							que sejam testados os diversos parâmentros utilizados pelo modelo PoseNet disponibilizado
							pela biblioteca <em>ml5.js</em>.<br>
							O código-fonte do experimento encontra-se disponível no
							<a href="https://github.com/dlucio/inf2064/blob/master/project/experiments/posenet/sketch.js" target="_blank" rel="noopener noreferrer">
								github do projeto
							</a>
						</p>
					</section>
					<section>
						<h4><a href="../experiments/poseshift/" target="_blank">PoseShift</a></h4>
						<p style="text-align: justify;">
							O experimento <a href="../experiments/poseshift/" target="_blank">PoseShift</a>
							é a integração entre os experimentos OpenCV: Meanshift & Camshift e PoseNet. <br>
							O código-fonte do experimento encontra-se disponível no
							<a href="https://github.com/dlucio/inf2064/blob/master/project/experiments/poseshift/sketch.js" target="_blank" rel="noopener noreferrer">
								github do projeto
							</a>
						</p>
					</section>
					<section>
						<h4><a href="../experiments/poseflow/" target="_blank">PoseFlow</a></h4>
						<p style="text-align: justify;">
							O experimento <a href="../experiments/poseflow/" target="_blank">PoseFlow</a>
							é a integração entre os experimentos OpenCV: Lucas-Kanade Optical Flow e PoseNet.<br>
							O código-fonte do experimento encontra-se disponível no
							<a href="https://github.com/dlucio/inf2064/blob/master/project/experiments/poseflow/sketch.js" target="_blank" rel="noopener noreferrer">
								github do projeto
							</a>
						</p>
					</section>					
				</section>
				
				<section>
					<h3>
						<a href="../posetracking/" target="_blank">Pose Tracking</a>
					</h3>
					<section data-menu-title="Pose Tracking">
						<p style="text-align: justify;">
							No projeto <a href="../posetracking/" target="_blank">Pose Tracking</a>  
							é utilizado o modelo 
							<a href="https://github.com/tensorflow/tfjs-models/tree/master/posenet" target="_blank">PoseNet</a> 
							para detectar a pose das diversas pessoas, 
							e para o rastreamento e identificação de cada pose detectada é utilizado o algoritmo 
							<a href="https://www.pyimagesearch.com/2018/07/23/simple-object-tracking-with-opencv/" target="_blank">Centroid Tracking</a>. 
						</p>
					</section>

					<section>
						<h5 style="text-align: left;">
							<a href="https://www.pyimagesearch.com/2018/07/23/simple-object-tracking-with-opencv/" target="_blank">Centroid Tracking</a>
						</h5>
					</section>
				</section>

				<!-- <section>
					<h4>Contribuições</h4>
					<ul style="font-size: 0.8em;">
						<li>Removido memory leak do ml5.PoseNet</li>
						<li>Adicionada bounding box ao ml5.PoseNet</li>
						<li>Primeiro exemplo utilizando ml5.PoseNet, OpenCV.js e p5js</li>
						<li>Corrigido os valores dos parâmetros de incialização do ml5.PoseNet.
							<ul>
								<li><em>Na documentação informava alguns valores mas no código os valores dos parâmentros eram outros</em></li> 							
							</ul>
						</li>
						<li>Adicionada a opção de habilitar e desabilitar a inferência no ml5.PoseNet.
							<ul>
								<li><em>Isso é útil quando é usada a interface de eventos para receber as poses detectadas.</em></li>
							</ul>
						</li>
					</ul>
				</section> -->

				<section>
					<h4>Trabalhos futuros</h4>
					<ul>
						<li>Utilizar o algoritmo Lucas-Kanade para fazer o tracking dos keypoints de diversas pessoas</li>
						<li>Utilizar o filtro de Kalman para dar mais estabilidade ao rastreamento</li>
					</ul>
				</section>
				
				<section>
					<h4>Conclusão</h4>
				</section>
			</div>
		</div>

		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				menu: {
					numbers: false,
					openSlideNumber: true,
					themes: true,
					themesPath: 'css/theme/',
					transitions: true,
				},
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true },
					{ src: 'plugin/reveal.js-menu/menu.js', async: true },
					// Zoom in and out with Alt+click
					{ src: 'plugin/zoom-js/zoom.js', async: true },
				]
			});
		</script>
	</body>
</html>
